{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkWZTE8K4Qh_"
   },
   "source": [
    "**Unzip the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nAG9ak6u4UMh"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"Sinhala_dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"Sinhala_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4qJzXb63Uqh"
   },
   "source": [
    "**Setting Up the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KuttTw8K2xll",
    "outputId": "a2c1c394-7750-409f-ac64-e9ff7b8a1ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 17796\n",
      "Total unique words: 36896\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Directory containing your .txt files\n",
    "data_dir = 'Sinhala_dataset/Sinhala_dataset'\n",
    "\n",
    "# List to store all sentences and words\n",
    "sentences = []\n",
    "word_list = []\n",
    "\n",
    "# Define a pattern to match only valid Sinhala words\n",
    "sinhala_word_pattern = r'[\\u0D80-\\u0DFF]+'\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove any unwanted characters, leaving only Sinhala words\n",
    "    clean_sentences = []\n",
    "    raw_sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "\n",
    "    for sentence in raw_sentences:\n",
    "        # Find all Sinhala words in the sentence\n",
    "        words = re.findall(sinhala_word_pattern, sentence)\n",
    "        if words:  # Avoid empty sentences\n",
    "            clean_sentences.append(\" \".join(words))\n",
    "            word_list.extend(words)  # Add words to the global list\n",
    "\n",
    "    return clean_sentences\n",
    "\n",
    "# Process each .txt file in the dataset directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            file_sentences = clean_and_tokenize(text)\n",
    "            sentences.extend(file_sentences)  # Add file sentences to global list\n",
    "\n",
    "# Create dictionary by counting unique words\n",
    "word_count = Counter(word_list)\n",
    "unique_words = sorted(word_count.keys())  # Sorted list of unique words for spelling checker\n",
    "\n",
    "# Save the unique words to a dictionary file for later use\n",
    "with open(\"sinhala_dictionary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in unique_words:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "# Output\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Total unique words: {len(unique_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2e6hhab3qYm"
   },
   "source": [
    "**Generating Sample Pairs of Misspelled and Correct Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C-vBL43i3ri8"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load dictionary of unique words\n",
    "with open(\"sinhala_dictionary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    unique_words = [line.strip() for line in f]\n",
    "\n",
    "# Generate misspelled-correct pairs\n",
    "misspelled_words = []\n",
    "correct_words = []\n",
    "\n",
    "# Simulate misspellings by altering one character in each word\n",
    "for word in unique_words[:10000]:\n",
    "    if len(word) > 1:\n",
    "        misspelled = list(word)\n",
    "        rand_index = random.randint(0, len(misspelled) - 1)\n",
    "        misspelled[rand_index] = random.choice(list(\"අආඇඈඉඊඋඌඑඒඔඕකගජටඩතදනපබමයරලවශසහෆ\"))\n",
    "        misspelled = ''.join(misspelled)\n",
    "        misspelled_words.append(misspelled)\n",
    "        correct_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfViEqROIT8P",
    "outputId": "983b3353-4415-4748-ac42-945e295069be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['අර',\n",
       " 'අංල',\n",
       " 'අංකඑ',\n",
       " 'ඩංකයකින්',\n",
       " 'අජකයට',\n",
       " 'අංකඋර',\n",
       " 'අංකසර',\n",
       " 'අංත',\n",
       " 'අංගණඇට',\n",
       " 'අදගනය']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspelled_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCXsxTzdDjMH"
   },
   "source": [
    "**Proceeding with Tokenization and Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEzNU_ibFisb",
    "outputId": "10ebd316-b6a5-4c73-937b-0fb7924f238c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yl_zvxLKDkf7",
    "outputId": "51c34e54-a66d-46fa-914f-c8b55dd09d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.6270 - loss: 1.9538 - val_accuracy: 0.7035 - val_loss: 1.1432\n",
      "Epoch 2/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.7242 - loss: 1.0167 - val_accuracy: 0.8635 - val_loss: 0.5998\n",
      "Epoch 3/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 69ms/step - accuracy: 0.8867 - loss: 0.5177 - val_accuracy: 0.9227 - val_loss: 0.3697\n",
      "Epoch 4/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 63ms/step - accuracy: 0.9292 - loss: 0.3554 - val_accuracy: 0.9390 - val_loss: 0.3092\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 67ms/step - accuracy: 0.9411 - loss: 0.3050 - val_accuracy: 0.9444 - val_loss: 0.2773\n",
      "Epoch 6/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9459 - loss: 0.2711 - val_accuracy: 0.9471 - val_loss: 0.2598\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - accuracy: 0.9478 - loss: 0.2543 - val_accuracy: 0.9490 - val_loss: 0.2478\n",
      "Epoch 8/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.9490 - loss: 0.2426 - val_accuracy: 0.9504 - val_loss: 0.2410\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.9510 - loss: 0.2339 - val_accuracy: 0.9504 - val_loss: 0.2350\n",
      "Epoch 10/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.9519 - loss: 0.2273 - val_accuracy: 0.9514 - val_loss: 0.2318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b1cabb3eec0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenize the words at the character level\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(misspelled_words + correct_words)\n",
    "sequences = tokenizer.texts_to_sequences(misspelled_words)\n",
    "\n",
    "# Pad sequences to ensure consistent input length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "y = tokenizer.texts_to_sequences(correct_words)\n",
    "y = pad_sequences(y, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model architecture\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=True), # Return sequences for each time step\n",
    "    # Add a Dense layer to output probabilities for each character in the vocabulary\n",
    "    Dense(vocab_size, activation='softmax') # Output layer with vocab_size units and softmax activation\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAoCiKza4HbD"
   },
   "source": [
    "**Saving the Model for Future Use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4_DU8wa4KVP",
    "outputId": "07065d32-d7b9-44ff-9bd6-2e8b5d7ad2f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('spelling_checker_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('spelling_checker_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
